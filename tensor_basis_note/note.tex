% \documentclass[11pt,twoside]{book} %纸质版用twoside
\documentclass[12pt,oneside]{book} %电子版用oneside
\usepackage{setspace}

% \documentclass{article}

\input{notes_template.tex}

%%%%%%%%%%%%%%%%%%% biblatex %%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%% glossaries %%%%%%%%%%%%%%%%%
\input{./glossaries.tex}
%%%%%%%%%%%%%%%%%%%%% glossaries %%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%% glossaries-extra %%%%%%%%%%%%%%%%%
% \usepackage[record,abbreviations,symbols,stylemods={list,tree,mcols}]{glossaries-extra}
%%%%%%%%%%%%%%%%%%%%% glossaries-extra %%%%%%%%%%%%%%%%%


\input{./macros.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%% begin of document %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{\bf \huge Study Notes of Numerical Optimization}
\author{Pei Zhong}
\date{Update on \today}

\maketitle

% \newpage
% \let\cleardoublepag\clearpage

\tableofcontents

\begin{spacing}{1}

%%%%%%%%%%%%%%update progress%%%%%%%%%%
\chapter*{Update progress}
\begin{itemize}
    \item {writing ch\ref{chp:Convex Optimization}... \hfill 2023.10.20}
\end{itemize}


%%%%%%%%%%%%%%update progress end%%%%%%%%




%%%%%%%%%%%%%%%preface%%%%%%%%%%%%%
\chapter*{Preface}


Notes mainly refer to the following resources:
\begin{itemize}
    \item \href{https://www-labs.iro.umontreal.ca/~grabus/courses/ift6760a-w20.html}{lecture notes from course "Matrix and tensor factorization techniques for machine learning"}
    \item \href{https://web.stanford.edu/class/cs168/}{lecture notes from course "The Modern Algorithmic Toolbox"}
\end{itemize}

\par

all the codes in the notes are completed by python. You can get the codes in the following website:
\begin{itemize}
    \item -
\end{itemize}


%%%%%%%%%%%%%preface end%%%%%%%%%%%%%

\part{Fundamental Concepts of Optimization}
%%%%%%%%%%%%%ch1 convex%%%%%%%%%%%%%%
\chapter{Convex Optimization}\label{chp:Convex Optimization}


\textit{"The great watershed in Optimization is not 
between linearity and nonlinearity, but convexity and nonconvexity. "}

%%%%%%%%%%%%ch1 section1%%%%%%%%%%%
\section{General Convex Optimization Problems}
\begin{definition}{
    (Convex Set) % 定理的名字
  }{% label
  }
    {
        a set \ $\Omega\in \R^n$ is convex if 
         \begin{equation}
            \forall x,y \in \Omega, t\in [0,1]: x+t(y-x)\in \Omega. 
         \end{equation}


        
    }
\end{definition}


\begin{remark}
    This definition is equivalent to saying that
    all connecting lines lie inside set. 
\end{remark}

\begin{figure}[htbp]
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figure/ch1/convex_set1.png}
        \caption{an example of a convex set}
    \end{minipage}%
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figure/ch1/non_convex_set1.png}
        \caption{an example of a non convex set}
    \end{minipage}
\end{figure}


\begin{definition}{
    (Convex Function) % 定理的名字
  }{% label
  }
    {
        a function $f: \Omega \rightarrow \R$ is convex, if $\Omega$ is convex and if
         \begin{equation}
            \forall x,y \in \Omega, t\in [0,1]: f(x+t(y-x))\leq f(x)+t(f(y)-f(x)). 
         \end{equation}
    }
\end{definition}

\begin{remark}
    This definition is equivalent to saying that
    all secants are above graph.
\end{remark}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{figure/ch1/secant_above_graph1.png}
    \caption{For a convex function, the line segment \\ between any two points on the graph lies
    above the graph}
\end{figure}

\begin{definition}{
    (Convex Optimizaiton Problem) % 定理的名字
  }{% label
  }
    {
        an optimization problem with 
        \begin{itemize}
            \item a convex feasible set $\Omega$ and 
            \item a convex objective function $f: \Omega\rightarrow\R$
        
        \end{itemize}
        is called a "convex Optimization problem"
    }
\end{definition}

\begin{theorem}{
    (Local Implies Global Optimality for Convex Problems)
}{}
    {
        for a convex Optimization problem, every local minimum is also a global one. 
    }
\end{theorem}

\begin{proofsolution}
   
    \textcolor{LightRed}{
        Consider a local minimum $x^*$ of the convex optimization problem
        \begin{equation}
            \begin{split}
                \min_{x\in \R^n} f(x)\\
                s.t. x\in \Omega \nonumber
            \end{split}
        \end{equation}
        We will show that for each y $\in \Omega$ it holds $f(y)\geq f(x^*)$. 
    }

    Suppose that $x^*$ is not the global minmium, that is $\exists \ \widetilde{x} \in \Omega \ s.t. \ f(\widetilde{x})<f(x^*)$. \par
    Consider the line segement $x(t)=tx^*+(1-t)\widetilde{x}, t\in [0,1]$, 
        noting that $x(t)\in \Omega$ by the convexity of $\Omega$. 
        By the convexity of $f$,
        \begin{equation*}
            f(x(t))\leq tf(x^*)+(1-t)f(\widetilde{x})<tf(x^*)+(1-t)f(x^*)=f(x^*),\forall t\in [0,1]
        \end{equation*}
    \par
    As $x^*$ is a local minmium, we know that $\exists N$($N$ is a neighbourhood of $x^*$), $\forall x\in N$, $f(x)\leq f(x^*)$. 
        We can pick $t$ sufficiently to $1$ such that $x(t) \in N$. Then $f(x(t)) < f(x^*)$. 
        This is a contradiction as $f(x(t))<f(x^*)$ by the above inequality.
    \par
    Hence, $x^*$ is the global minimum. 
\end{proofsolution}

%%%%%%%%%ch1 section1 end%%%%%%%%

%%%%%%%%%ch2 section2%%%%%%%%%%%%

\section{How to Check Convexity of Functions?}

\begin{theorem}{
    (Local Implies Global Optimality for Convex Problems)
}{}
    {
        for a convex Optimization problem, every local minimum is also a global one. 
    }
\end{theorem}

%%%%%%%%

%%%%%%%%
\section{convex optimization problems}


%%%%%%%%%%%%%ch1 end%%%%%%%%%%%%%%


%%%%%%%%%%%%ch2 linear progrmming%%%%%%%

\chapter{Linear Programming}
\section{Introduction to LP}

\section{LP Relaxation}


\section{Algorithms Solving LP}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%part 2 




%%%%%%%%%%%%%%%Content%%%%%%%%%%%%%%%
% % \mainmatter % separat the number of toc and mainmatter
% \input{./chapter/preface.tex}

% \part{Mathematics}

% \input{./chapter/discrete_math.tex}

% \part{Computer Science}
% % \input{./chapter/complexity.tex}
% \input{./chapter/machine_learning.tex}
% % \input{./chapter/algorithms.tex}

% \part{Physics}
% \input{./chapter/quantum_mechanics.tex}
% % \input{./chapter/quantum_field_theory.tex}

% % \begin{appendices}
% % \input{./chapter/appendix_formula.tex}
% % \end{appendices}

% \backmatter

% %%%%%%%%%%%%%%% Reference %%%%%%%%%%%%%%%

% \printbibliography[heading=bibintoc]
% \printindex
\end{spacing}
\end{document}