\chapter{Matrix Algebra}

\section{Notations and definitions}

Scalars, column vectors, matrices, and hypermatrices/tensors of order higher than two will be 
denoted by lowercase letters $(a,b,...)$, 
bold lowercase letters $(\bold{a,b},...)$,
bold uppercase letters $(\bold{A},\bold{B},...)$,
and calligraphic letters $(\mathcal{A},\mathcal{B},...)$ respectively.

\par
A matrix $\bold{A}$ of dimensions $I\times J$, with $I$ and $J\in \N^*$,
denoted by $\bold{A}(I,J)$, is an array of $IJ$ elements stored in $I$ rows and 
$J$ columns; the elements belong to a field $\K$. Its $i$th row and
$j$th column, denoted by $A_{i\cdot}$ and $A_{\cdot j}$, respectively, are called $i$th row vector and
$j$th column vector. The element located at the intersection of $A_{i\cdot}$ and $A_{\cdot j}$ is 
designated by $a_{ij}$. We will use the notation $\bold{A}=(a_{ij})$, 
with $a_{ij}\in\K$, $i\in \langle {I} \rangle=\{1,2,...,I\}$ and 
$j\in \langle{J} \rangle=\{1,2,...,J\}$.
\par
A matrix $A\in\K^{I\times J}$ is written in the form:
\begin{align*}
    \begin{pmatrix}
        a_{11}& a_{12} & ... & a_{1J} \\
        a_{12}& a_{22} & ... & a_{2J} \\
        ...& ...  & ... & ...\\
        a_{I1}& a_{I2}  & ... & a_{IJ}
      \end{pmatrix}
\end{align*} 
The special cases $I=1$ and $J=1$ correspond respectively to row vectors of dimension $J$ 
and to column vectors of dimension $I$:
\begin{align*}
    \bold{v}=\begin{pmatrix}
        v_1& v_{2} & ... & v_{J} 
      \end{pmatrix}\in \K^{1\times J},
    \bold{u}=\begin{pmatrix}
        u_1\\ u_{2} \\ ... \\ u_{I} 
      \end{pmatrix}\in \K^{I\times 1}.
\end{align*}
In the following, for column vectors, $\K^{I}$ will be used instead of $\K^{I\times 1}$.
\par
Identity matrix (i.e., a matrix with all 1's on the diagonal
and 0's everywhere else) is denoted by $\bold{E}$.
$\bold{e}_{i}^{(I)}$ is the column vector of dimension $I$,
in which element is equal to $1$ at position $i$ and $0s$ elsewhere.
$\bold{E}_{ij}^{I\times J}$ is the matrix of dimension $I\times J$
in which element is equal to $1$ at position $(i,j)$ and $0s$ elsewhere.

\section{Transposition and conjugate transposition}
\begin{definition}{transpose and the conjugate transpose of a column vector}{transpose and the conjugate transpose of a column vector}
    The transpose and the conjugate transpose (also called transconjugate) of a column
    vector $\bold{u}=\begin{pmatrix}u_1\\ u_{2} \\ ... \\ u_{I} \end{pmatrix}\in \C^{I}$, 
    denoted by $\bold{u}^{T}$ and $\bold{u}^{H}$, respectively, are the row vectors defined as:
    \begin{align*}
        \bold{u}^T=\begin{pmatrix}u_1\\ u_{2} \\ ... \\ u_{I} \end{pmatrix}\text{ and } 
        \bold{u}^H=\begin{pmatrix}u_1^*& u_{2}^* & ... & u_{I}^* \end{pmatrix},
    \end{align*}
    where $u_i^*$ is the conjugate of $u_i$ also denoted by $\overline{u}_i$.
\end{definition}

\begin{definition}{transpose and the conjugate transpose of a matrix}{transpose and the conjugate transpose of a matrix}
    The transpose of $\bold{A}\in\K^{I\times J}$ is the matrix denoted by $\bold{A}^T$, of dimensions $J\times I$, such that $A^T=(a_{ji})$, 
    with $i\in\langle {I}\rangle$ and $j\in\langle {J}\rangle$.
    In the case of a complex matrix, the conjugate transpose, also known as Hermitian transpose and denoted by $\bold{A}^H$,
    is defined as: $\bold{A}^{H}=(\bold{A}^*)^T=(\bold{A}^T)^*=(a_{ji}^*)$,
    where $\bold{A}^*=(a_{ij}^*)$ is the conjugate of $\bold{A}$.
\end{definition}

\begin{remark}
    In mathematics, the complex conjugate of a complex number 
    is the number with an equal real part and 
    an imaginary part equal in magnitude but opposite in sign. 
    That is, if $a$ and $b$ are real numbers 
    then the complex conjugate of $a+\text{i}b$ is $a-\text{i}b$.
    The complex conjugate of $z$ is often denoted as $\bar{z}$ or $z^*$.
\end{remark}


\begin{proposition}{}{}
    The operations of transposition and conjugate transposition satisfy:
    \begin{align*}
        (\bold{A}^T)^T=\bold{A}&, (\bold{A}^H)^H=\bold{A}\\
        (\bold{A}+\bold{B})=\bold{A}^T+\bold{B}^T&, (\bold{A}+\bold{B})^{H}=\bold{A}^H+\bold{B}^H,\\
        (\alpha \bold{A})^T=\alpha \bold{A}^T&, (\alpha \bold{A})^H=\alpha^*\bold{A}^H, 
    \end{align*}
    for any matrix $\bold{A},\bold{B}\in\C^{I\times J}$ and any scalar $\alpha\in\C$.
\end{proposition}

\begin{remark}
    By decomposing $\bold{A}$ using its real and imaginary parts, we have:
    \begin{align*}
        \bold{A}=\text{Re}(\bold{A}) +\text{i} \text{Im} (\bold{A}) \Rightarrow 
        \left\{\begin{matrix}
           \bold{A}^T = (\text{Re}(\bold{A}))^T+\text{i} (\text{Im}(A))^T \\
           \bold{A}^H = (\text{Re}(\bold{A}))^H-\text{i} (\text{Im}(A))^H
          \end{matrix}\right.
    \end{align*}.
\end{remark}


\section{Vector outer product and vectorization}
\subsection{Vector outer product}
The outer product of two vectors $\bold{u}\in\K^{I}$ and $\bold{v}\in \K^{J}$,
denoted $\bold{u}\circ \bold{v}$,
gives a matrix $\bold{A}\in \K^{I\times J}$
such that $a_{ij}=(\bold{u}\circ\bold{v})_{ij}=u_iv_j$,
and therefore, $\bold{u}\circ\bold{v}=\bold{u}\bold{v}^T=(u_iv_j)$, 
with $i\in \langle I\rangle,j\in\langle J\rangle$.

\begin{example}{}{}
    For $I=2,J=3$, we have:
    \begin{align*}
        \begin{pmatrix}
            u_1\\ u_{2}
        \end{pmatrix}\circ
        \begin{pmatrix}
            v_1\\ v_{2} \\v_{3}
        \end{pmatrix}
        = \begin{pmatrix}
            u_1\\ u_{2}
        \end{pmatrix} 
        \begin{pmatrix}
            v_1& v_{2} &v_{3}
        \end{pmatrix}
        = \begin{pmatrix}
            u_1v_1& u_1v_{2} & u_1v_{3}\\
            u_2v_1& u_2v_2 & u_2v_3
        \end{pmatrix}.
    \end{align*}
\end{example}

\subsection{Vectorization}
A very widely used operation in matrix computation is vectorization
which consists of stacking the columns of a matrix $\bold{A}\in \K^{I\times J}$
on top of each other to form a column vector of dimension $JI$:
\begin{align*}
    \bold{A}=\begin{pmatrix}
        \bold{A}_{\cdot 1}& \bold{A}_{\cdot 2} & ... & \bold{A}_{\cdot J}
    \end{pmatrix}\in \K^{I\times J}
    \Rightarrow \text{vec}(\bold{A}) = \begin{pmatrix}
        \bold{A}_{\cdot 1} \\ \bold{A}_{\cdot 2}\\ ...\\ \bold{A}
        _{\cdot J}
    \end{pmatrix}\in \K^{JI}.
\end{align*}
This operation defines an isomorphism between the space $\K^{JI}$ of vectors
of dimension $JI$ and the space $\K^{I\times J}$ of matrices $I\times J$.
Indeed, the canonical basis of $\K^{JI}$, denoted by $\{\bold{e}_{(j-1)I+i}^{(JI)}\}$, 
allows us to write $\text{vec}(\bold{A})$ as:
\begin{align*}
    \bold{A}=\sum\limits_{i=1}^{I}\sum\limits_{j=1}^{J}a_{ij}\bold{e}_i^{(I)}\circ \bold{e}_j^{(J)}
    \Rightarrow
    \text{vec}(\bold{A})=\sum\limits_{i=1}^{I}\sum\limits_{j=1}^{J}a_{ij}\bold{e}_{(j-1)I+i}^{(JI)},
\end{align*}
with $\bold{e}_{(j-1)I+i}^{(JI)}=\text{vec}(\bold{e}_i^{(I)}\circ \bold{e}_j^{(J)})=\text{vec}(\bold{e}_i^{(I)}(\bold{e}_j^{(J)})^T)$.

\begin{remark}
    Since the operator $\text{vec}$ satisfies $\text{vec}(\alpha \bold{A}+\beta \bold{B})=\alpha \text{vec}(\bold{A})+\beta \text{vec}(\bold{B})$
    for all $\alpha,\beta\in\K$, it is linear.
\end{remark}


\section{Vector inner product and orthogonality}
\subsection{Inner product}

In this section, we recall the definition of the inner product(also called dot product)
of two vectors $\bold{a},\bold{b}\in\K^{I}$.
\begin{definition}{}{}
    If $\K=\R$, the vector inner product is defined as:
    \begin{align*}
        \inner{\cdot}{\cdot}:\R^I\times \R^I&\rightarrow \R\\
        (\bold{a},\bold{b})&\mapsto \inner{\bold{a}}{\bold{b}} = \bold{a}^T\bold{b}=\sum\limits_{i=1}^{I}a_ib_i.
    \end{align*}
    In $\C^I$, the definition of the vector inner product is given by:
    \begin{align*}
        \inner{\cdot}{\cdot}:\C^I\times \C^I&\rightarrow \C\\
        (\bold{a},\bold{b})&\mapsto \inner{\bold{a}}{\bold{b}} = \bold{a}^H\bold{b}=\sum\limits_{i=1}^{I}a_i^*b_i.
    \end{align*}
\end{definition}
\begin{remark}
    Whether $\K=\C$ or $\R$, $\inner{\bold{a}}{\bold{a}}\in \R$. 
\end{remark}


\subsection{Orthogonality}
\begin{definition}{}{}
    Two vectors $\bold{a}$ and $\bold{b}$ of $\K^{I}$ are said to be 
    orthogonal if and only if $\inner{\bold{a}}{\bold{b}}=0$. 
\end{definition}


\section{Vector Norms}
\begin{definition}{}{}
    Let $v:\C^n\rightarrow \R$. 
    Then $v$ is a norm if for all $\bold{x},\bold{y}\in\C^n$
    \begin{itemize}
        \item $\bold{x}\neq 0\Rightarrow v(\bold{x})>0$,
        \item $v(\alpha \bold{x})=|\alpha|v(\bold{x})$, and
        \item $v(\bold{x}+\bold{y})\leqs v(\bold{x})+v(\bold{y})$ 
    \end{itemize}
\end{definition}
\begin{remark}
    often we will use $||\cdot||$ to denote a vector norm.
\end{remark}


\subsection{Vector 2-norm}
\begin{definition}{}{}
    The vector 2-norm $||\cdot||_2:\C^n\rightarrow \R$ is defined by
    \begin{align*}
        ||\bold{x}||_2=\sqrt{\inner{\bold{x}}{\bold{x}}}=\sqrt{\bold{x}^H\bold{x}}=\sqrt{x_1^*x_1+x_2^*x_2+...+x_n^*x_n}=\sqrt{|x_1|^2+...+|x_n|^2}.
    \end{align*}
\end{definition}

\begin{theorem}{}{}
    Let $\bold{x},\bold{y}\in\C^n$. Then $|\inner{\bold{x}}{\bold{y}}|=|\bold{x}^H\bold{y}|\leqs ||\bold{x}||_2\cdot ||\bold{y}||_2$.
\end{theorem}

\begin{proposition}{}{}
    The vector 2-norm is a norm.
\end{proposition}

\subsection{Vector 1-norm}
\begin{definition}{}{}
    The vector 1-norm $||\cdot||_1:\C^n\rightarrow \R$ is defined by
    \begin{align*}
        ||\bold{x}||_1=|x_1|+|x_2|+...+|x_{n}|.
    \end{align*}
\end{definition}

\begin{proposition}{}{}
    The vector 1-norm is a norm.
\end{proposition}

\subsection{Vector $\infty$-norm}
\begin{definition}{}{}
    The vector $\infty$-norm $||\cdot||_{\infty}:\C^n\rightarrow \R$ is defined by $||\bold{x}||_{\infty}=\max_{i}|x_i|$.
\end{definition}

\begin{proposition}{}{}
    The vector $\infty$-norm is a norm.
\end{proposition}

\subsection{Vector $p$-norm}
\begin{definition}{}{}
    The vector $p$-norm $||\cdot||_p:\C^n\rightarrow \R$ is defined by
    \begin{align*}
        ||\bold{x}||_p=\sqrt[p]{|x_1|^p+|x_2|^p+...+|x_n|^p}.
    \end{align*}
\end{definition}

\begin{proposition}{}{}
    The vector $p$-norm is a norm.
\end{proposition}

\section{Matrix Norms}
It is not hard to see that vector norms are
all measures of how "big" the vectors are. Similarly,
we want to have measures for how "big" matrices are.
We will start with one that are somewhat artificial and then move on to the important class of induced matrix norms.

\subsection{Frobenius norm}
\begin{definition}{}{}
    The Frobenius norm $||\cdot||_F:\C^{m\times n}\rightarrow \R$ is defined by
    \begin{align*}
        ||\bold{A}||_F=\sqrt{\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{n}|a_{ij}|^2}.
    \end{align*}
\end{definition}

\begin{remark}
    $||\bold{A}||_F=||\text{vec}(\bold{A})||_2$.
\end{remark}

\begin{proposition}{}{}
    The Frobenius norm is a norm.
\end{proposition}

\subsection{Induced matrix norms}
\begin{definition}{}{}
    Let $||\cdot||_{\mu}:\C^m\rightarrow \R$ and $||\cdot||_{v}:\C^n\rightarrow \R$ be vector norms.
    Define $||\cdot||_{\mu,v}:\C^{m\times n}\rightarrow \R$ by 
    \begin{align*}
        ||\bold{A}||_{\mu,v}=\sup_{\bold{x}\in\C^n,\bold{x}\neq 0}\frac{||\bold{A\bold{x}}||_{\mu}}{||\bold{x}||_v}.
    \end{align*}
\end{definition}

\begin{remark}
    How "big" $\bold{A}$ is, as measured by $||\bold{A}||_{\mu,v}$, 
    is defined as the most that $\bold{A}$ magnifies the length of nonzero vectors,
    where the length of the vector $\bold{x}$ is measured with norm $||\cdot||_v$ and
    the length of the transformed vector $\bold{A}\bold{x}$ is measured with norm $||\cdot||_{\mu}$.
\end{remark}

\begin{proposition}{}{}
    Let $||\cdot||_{\mu}:\C^m\rightarrow \R$ and $||\cdot||_{v}:\C^n\rightarrow \R$ be vector norms.
    \begin{align*}
        ||\bold{A}||_{\mu,v}&=\sup_{\bold{x}\in\C^n,\bold{x}\neq 0}\frac{||\bold{A\bold{x}}||_{\mu}}{||\bold{x}||_v}\\
                            &=\max_{\bold{x}\in\C^n,\bold{x}\neq 0}\frac{||\bold{A\bold{x}}||_{\mu}}{||\bold{x}||_v}\\
                            &=\max_{||\bold{x}||_v=1}||\bold{A}\bold{x}||_{\mu}.
    \end{align*}
\end{proposition}

\begin{proposition}{}{}
    $||\cdot||_{\mu,v}:\C^{m\times n}\rightarrow \R$ is a norm. 
\end{proposition}

\begin{definition}{}{}
    Define $||\cdot||_p:\C^{m\times n}\rightarrow \R$ by
    \begin{align*}
        ||\bold{A}||_{p}&=\max_{\bold{x}\in\C^n,\bold{x}\neq 0}\frac{||\bold{A\bold{x}}||_{p}}{||\bold{x}||_p}\\
                            &=\max_{||\bold{x}||_p=1}||\bold{A}\bold{x}||_{p}.
    \end{align*}
\end{definition}


\begin{proposition}{}{}
    For any $\bold{A}\in \C^{m\times k}$ and $\bold{B}\in\C^{k\times n}$, 
    \begin{align*}
        ||\bold{A}\bold{B}||_p&\leqs ||\bold{A}||_p||\bold{B}||_p\\
        ||\bold{A}\bold{B}||_F&\leqs ||\bold{A}||_F||\bold{B}||_F.
    \end{align*}
\end{proposition}

\section{Matrix multiplication}
Suppose $\bold{A}\in\R^{m\times r}$ and $\bold{B}\in \R^{r\times n}$,
the matrix multiplication $\bold{C}=\bold{A}\bold{B}$ can be viewed from
three different perspectives as follows:
\par
\underline{Dot Product Matrix Multiply}. 
Every element $c_{ij}$ of $\bold{C}$ is the 
dot product of row vector $\bold{A}_{i\cdot}$ and column vector $\bold{B}_{\cdot j}$.
\begin{align*}
    \bold{A}&=\begin{pmatrix}
        \bold{A}_{1\cdot}^T\\ ...\\\bold{A}_{m\cdot}^T
    \end{pmatrix},\bold{A}_{k\cdot}\in\R^r\\
    \bold{B}&=\begin{pmatrix}
        \bold{B}_{\cdot 1}&...\bold{B}_{\cdot n}
    \end{pmatrix},\bold{B}_{\cdot k}\in\R^r\\
    \bold{C}&=(c_{ij}),c_{ij}=\bold{A}_{i\cdot}^T\bold{B}_{\cdot j}=\sum\limits_{k=1}^{r}a_{ik}b_{kj}.
\end{align*}
\par
\underline{Column Combination Matrix Multiply}.
Every column $\bold{C}_{\cdot j}$ of $\bold{C}$ is a linear combination of column vector
$\bold{A}_{\cdot k}$ of $\bold{A}$ with columns $b_{kj}$ as the weight coefficients.






\section{Matrix trace, Matrix inner product}

\subsection{Definition and properties of the trace}
\begin{definition}{}{}
    The trace of a square matrix $\bold{A}$ of order $I$ is defined as the sum of its diagonal elements:
    \begin{align*}
        \text{tr}(\bold{A})=\sum\limits_{i=1}^{I}a_{ii}.
    \end{align*}
\end{definition}

\begin{proposition}{}{}
    The trace satisfies the following properties:
    \begin{align*}
        \text{tr}(\alpha \bold{A}+\beta \bold{B}) &= \alpha \text{tr}(\bold{A}) + \beta \text{tr}(\bold{B}),\\
        \text{tr}(\bold{A}^T)&=\text{tr}(\bold{A}),\\
        \text{tr}(\bold{A}^*)&=\text{tr}(\bold{A}^H)=(\text{tr}(\bold{A}))^*,\\
    \end{align*}
\end{proposition}

\section{Subspaces associated with a matrix}

\section{Matrix rank}

\section{Determinant, inverses and generalized inverses}

\section{Quadric form, symmetric, positive definte, eigenvalues, eigenvectors and conics}
\begin{definition}{}{}
    A symmetric matrix is a square matrix $\bold{Q}\in\R^{n\times n}$ with the property that $\bold{Q}^T=\bold{Q}$.
\end{definition}

\begin{definition}{}{}
    $\bold{Q}$ is symmetric and positive semidefinite (abbreviated SPSD and denoted by $\bold{Q}\succeq 0$) if 
    \begin{align*}
        \bold{x}^T\bold{Q}\bold{x}\geqs 0 \text{ for all } \bold{x}\in \R^n.
    \end{align*}
\end{definition}

\begin{definition}{}{}
    $\bold{Q}$ is symmetric and positive semidefinite (abbreviated SPD and denoted by $\bold{Q}\succ 0$) if 
    \begin{align*}
        \bold{x}^T\bold{Q}\bold{x}> 0 \text{ for all } \bold{x}\in \R^n, \bold{x}\neq 0.
    \end{align*}
\end{definition}

A matrix $\bold{M}$ is an orthonormal matrix if $\bold{M}^T=\bold{M}^{-1}$.
Note that if $\bold{M}$ is orthonormal and $\bold{y}=\bold{M}\bold{x}$, then 
\begin{align*}
    ||\bold{y}||^2_2=\bold{y}^T\bold{y}=\bold{x}^T\bold{M}^T\bold{M}\bold{x}=\bold{x}^T\bold{M}^{-1}\bold{M}\bold{x}=\bold{x}^T\bold{x}=||\bold{x}||^2_2,
\end{align*}
and so $||\bold{y}||_2^2=||\bold{x}||_2^2$.

A number $\lambda\in\R$ is an eigenvalue of $\bold{M}$ if there exists 
a vector $\bold{\bar{x}}\neq 0$ such that $\bold{M}\bold{\bar{x}}=\lambda \bold{\bar{x}}$.
$\bold{\bar{x}}$ is called an eigenvector of $\bold{M}$ (and is called an eigenvector corresponding to $\lambda$).
Note that $\lambda$ is an eigenvalue of $\bold{M}$ if and only if there exists $\bold{\bar{x}}\neq 0$ such that 
$(\bold{M}-\lambda \bold{E})\bold{\bar{x}}=0$ or, equivalently, if and only if $\text{det}(\bold{M}-\lambda \bold{E})=0$.

Let $g(\lambda)=\text{det}(\bold{M}-\lambda \bold{E})$. Then $g(\lambda)$ is 
a polynomial of degree $n$, and so will have $n$ roots 
that will solve the equation $g(\lambda)=\text{det}(\bold{M}-\lambda \bold{E})=0$, 
including multiplicities. These roots are the eigenvalues of $\bold{M}$.


\begin{proposition}{}{}
    If $\bold{Q}$ is a real symmetric matrix, all of its eigenvalues are real numbers.
\end{proposition}

\begin{proposition}{}{}
    If $\bold{Q}$ is a real symmetric matrix, 
    its eigenvectors corresponding to different eigenvalues are orthogonal.
\end{proposition}

\begin{proposition}{}{}
    If $\bold{Q}$ is SPSD, the eigenvalues of $\bold{Q}$ are nonnegative.
\end{proposition}

\begin{theorem}{The Principal Axis Theorem}{}
    If $\bold{Q}$ is symmetric, then $\bold{Q}=\bold{R}\bold{D}\bold{R}^T$,
    where $\bold{R}$ is an orthonormal matrix, the columns of $\bold{R}$ are an orthonormal basis of eigenvectors of $\bold{Q}$,
    and $\bold{D}$ is a diagonal matrix of the corresponding eigenvalues of $\bold{Q}$.
\end{theorem}

\begin{proposition}{}{}
    If $\bold{D}$ is SPSD, the $\bold{Q}=\bold{M}^T\bold{M}$ for some matrix $\bold{M}$.
\end{proposition}


\begin{proposition}{}{}
    If $\bold{M}$ is $n\times n$ and symmetric, then
    \begin{align*}
        ||\bold{M}||_2=\max_{\lambda} \{|\lambda|:\lambda \text{ is an eigenvalue of } \bold{M}\}.
    \end{align*}
\end{proposition}

\begin{proposition}{}{}
    If $\bold{M}$ is $m\times n$ with $m<n$ and $\bold{M}$ has rank $m$, then 
    \begin{align*}
        ||\bold{M}||_2=\sqrt{\lambda_{\max}(\bold{M}\bold{M}^T)},
    \end{align*}
    where $\lambda_{\max}(\bold{A})$ denotes the largest eigenvalue of a matrix $\bold{A}$.
\end{proposition}

\begin{proposition}{}{}
    Suppose that $\bold{M}$ is a symmetric matrix. Then the following are equivalent:
    \begin{align*}
        &\text{(a) } h>0 \text{ satisfies } ||\bold{M}^{-1}||_2\leqs \frac{1}{h}.\\
        &\text{(b) } h>0 \text{ satisfies } ||\bold{M}\bold{x}||_2 \geqs h\cdot ||\bold{x}||_2 \text{ for any vector } \bold{x}\\
        &\text{(c) } h>0 \text{ satisfies } |\lambda_i(\bold{M})|\geqs h \text{ for every eigenvalue } \lambda_i(\bold{M}) \text{ of } \bold{M}, i=1,...,m.
    \end{align*}
\end{proposition}



\section{Reference}
\begin{itemize}
    \item \href{}{From Algebraic Structures to Tensors ch4 matrix algebra}
    \item \href{https://www.cs.utexas.edu/users/flame/Notes/NotesOnNorms.pdf}{Notes on Vector and Matrix Norms}
    \item \href{https://ocw.mit.edu/courses/15-084j-nonlinear-programming-spring-2004/resources/lec4_quad_form/}{symmetric, positive definted, eigenvalues and eigenvectors}
    \item \href{https://www.math.uwaterloo.ca/~jmckinno/Math225/Week7/Lecture2m.pdf}{The Principal Axis Theorem}
    \item \href{https://staff.polito.it/ada.boralevi/didattica/Dispense_ENG.pdf}{linear algebra and geometry note}
    \item \href{http://jde27.uk/la/36_eigenapplications2.html}{eigenvalues application: ellipses}
\end{itemize}
