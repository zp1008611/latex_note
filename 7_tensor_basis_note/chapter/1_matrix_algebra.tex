\chapter{Matrix Algebra}

\section{Notations and definitions}

Scalars, column vectors, matrices, and hypermatrices/tensors of order higher than two will be 
denoted by lowercase letters $(a,b,...)$, 
bold lowercase letters $(\bold{a,b},...)$,
bold uppercase letters $(\bold{A},\bold{B},...)$,
and calligraphic letters $(\mathcal{A},\mathcal{B},...)$ respectively.

\par
A matrix $\bold{A}$ of dimensions $I\times J$, with $I$ and $J\in \N^*$,
denoted by $\bold{A}(I,J)$, is an array of $IJ$ elements stored in $I$ rows and 
$J$ columns; the elements belong to a field $\K$. Its $i$th row and
$j$th column, denoted by $A_{i\cdot}$ and $A_{\cdot j}$, respectively, are called $i$th row vector and
$j$th column vector. The element located at the intersection of $A_{i\cdot}$ and $A_{\cdot j}$ is 
designated by $a_{ij}$. We will use the notation $\bold{A}=(a_{ij})$, 
with $a_{ij}\in\K$, $i\in \langle {I} \rangle=\{1,2,...,I\}$ and 
$j\in \langle{J} \rangle=\{1,2,...,J\}$.
\par
A matrix $A\in\K^{I\times J}$ is written in the form:
\begin{align*}
    \begin{pmatrix}
        a_{11}& a_{12} & ... & a_{1J} \\
        a_{12}& a_{22} & ... & a_{2J} \\
        ...& ...  & ... & ...\\
        a_{I1}& a_{I2}  & ... & a_{IJ}
      \end{pmatrix}
\end{align*} 
The special cases $I=1$ and $J=1$ correspond respectively to row vectors of dimension $J$ 
and to column vectors of dimension $I$:
\begin{align*}
    \bold{v}=\begin{pmatrix}
        v_1& v_{2} & ... & v_{J} 
      \end{pmatrix}\in \K^{1\times J},
    \bold{u}=\begin{pmatrix}
        u_1\\ u_{2} \\ ... \\ u_{I} 
      \end{pmatrix}\in \K^{I\times 1}.
\end{align*}
In the following, for column vectors, $\K^{I}$ will be used instead of $\K^{I\times 1}$.
\par
Identity matrix (i.e., a matrix with all 1's on the diagonal
and 0's everywhere else) is denoted by $\bold{E}$.
$\bold{e}_{i}^{(I)}$ is the column vector of dimension $I$,
in which element is equal to $1$ at position $i$ and $0s$ elsewhere.
$\bold{E}_{ij}^{I\times J}$ is the matrix of dimension $I\times J$
in which element is equal to $1$ at position $(i,j)$ and $0s$ elsewhere.

\section{Transposition and conjugate transposition}
\begin{definition}{transpose and the conjugate transpose of a column vector}{transpose and the conjugate transpose of a column vector}
    The transpose and the conjugate transpose (also called transconjugate) of a column
    vector $\bold{u}=\begin{pmatrix}u_1\\ u_{2} \\ ... \\ u_{I} \end{pmatrix}\in \C^{I}$, 
    denoted by $\bold{u}^{T}$ and $\bold{u}^{H}$, respectively, are the row vectors defined as:
    \begin{align*}
        \bold{u}^T=\begin{pmatrix}u_1\\ u_{2} \\ ... \\ u_{I} \end{pmatrix}\text{ and } 
        \bold{u}^H=\begin{pmatrix}u_1^*& u_{2}^* & ... & u_{I}^* \end{pmatrix},
    \end{align*}
    where $u_i^*$ is the conjugate of $u_i$ also denoted by $\overline{u}_i$.
\end{definition}

\begin{definition}{transpose and the conjugate transpose of a matrix}{transpose and the conjugate transpose of a matrix}
    The transpose of $\bold{A}\in\K^{I\times J}$ is the matrix denoted by $\bold{A}^T$, of dimensions $J\times I$, such that $A^T=(a_{ji})$, 
    with $i\in\langle {I}\rangle$ and $j\in\langle {J}\rangle$.
    In the case of a complex matrix, the conjugate transpose, also known as Hermitian transpose and denoted by $\bold{A}^H$,
    is defined as: $\bold{A}^{H}=(\bold{A}^*)^T=(\bold{A}^T)^*=(a_{ji}^*)$,
    where $\bold{A}^*=(a_{ij}^*)$ is the conjugate of $\bold{A}$.
\end{definition}

\begin{remark}
    In mathematics, the complex conjugate of a complex number 
    is the number with an equal real part and 
    an imaginary part equal in magnitude but opposite in sign. 
    That is, if $a$ and $b$ are real numbers 
    then the complex conjugate of $a+\text{i}b$ is $a-\text{i}b$.
    The complex conjugate of $z$ is often denoted as $\bar{z}$ or $z^*$.
\end{remark}


\begin{proposition}{}{}
    The operations of transposition and conjugate transposition satisfy:
    \begin{align*}
        (\bold{A}^T)^T=\bold{A}&, (\bold{A}^H)^H=\bold{A}\\
        (\bold{A}+\bold{B})=\bold{A}^T+\bold{B}^T&, (\bold{A}+\bold{B})^{H}=\bold{A}^H+\bold{B}^H,\\
        (\alpha \bold{A})^T=\alpha \bold{A}^T&, (\alpha \bold{A})^H=\alpha^*\bold{A}^H, 
    \end{align*}
    for any matrix $\bold{A},\bold{B}\in\C^{I\times J}$ and any scalar $\alpha\in\C$.
\end{proposition}

\begin{remark}
    By decomposing $\bold{A}$ using its real and imaginary parts, we have:
    \begin{align*}
        \bold{A}=\text{Re}(\bold{A}) +\text{i} \text{Im} (\bold{A}) \Rightarrow 
        \left\{\begin{matrix}
           \bold{A}^T = (\text{Re}(\bold{A}))^T+\text{i} (\text{Im}(A))^T \\
           \bold{A}^H = (\text{Re}(\bold{A}))^H-\text{i} (\text{Im}(A))^H
          \end{matrix}\right.
    \end{align*}.
\end{remark}


\section{Vector outer product and vectorization}
\subsection{Vector outer product}
The outer product of two vectors $\bold{u}\in\K^{I}$ and $\bold{v}\in \K^{J}$,
denoted $\bold{u}\circ \bold{v}$,
gives a matrix $\bold{A}\in \K^{I\times J}$
such that $a_{ij}=(\bold{u}\circ\bold{v})_{ij}=u_iv_j$,
and therefore, $\bold{u}\circ\bold{v}=\bold{u}\bold{v}^T=(u_iv_j)$, 
with $i\in \langle I\rangle,j\in\langle J\rangle$.

\begin{example}{}{}
    For $I=2,J=3$, we have:
    \begin{align*}
        \begin{pmatrix}
            u_1\\ u_{2}
        \end{pmatrix}\circ
        \begin{pmatrix}
            v_1\\ v_{2} \\v_{3}
        \end{pmatrix}
        = \begin{pmatrix}
            u_1\\ u_{2}
        \end{pmatrix} 
        \begin{pmatrix}
            v_1& v_{2} &v_{3}
        \end{pmatrix}
        = \begin{pmatrix}
            u_1v_1& u_1v_{2} & u_1v_{3}\\
            u_2v_1& u_2v_2 & u_2v_3
        \end{pmatrix}.
    \end{align*}
\end{example}

\subsection{Vectorization}
A very widely used operation in matrix computation is vectorization
which consists of stacking the columns of a matrix $\bold{A}\in \K^{I\times J}$
on top of each other to form a column vector of dimension $JI$:
\begin{align*}
    \bold{A}=\begin{pmatrix}
        \bold{A}_{\cdot 1}& \bold{A}_{\cdot 2} & ... & \bold{A}_{\cdot J}
    \end{pmatrix}\in \K^{I\times J}
    \Rightarrow \text{vec}(\bold{A}) = \begin{pmatrix}
        \bold{A}_{\cdot 1} \\ \bold{A}_{\cdot 2}\\ ...\\ \bold{A}
        _{\cdot J}
    \end{pmatrix}\in \K^{JI}.
\end{align*}
This operation defines an isomorphism between the space $\K^{JI}$ of vectors
of dimension $JI$ and the space $\K^{I\times J}$ of matrices $I\times J$.
Indeed, the canonical basis of $\K^{JI}$, denoted by $\{\bold{e}_{(j-1)I+i}^{(JI)}\}$, 
allows us to write $\text{vec}(\bold{A})$ as:
\begin{align*}
    \bold{A}=\sum\limits_{i=1}^{I}\sum\limits_{j=1}^{J}a_{ij}\bold{e}_i^{(I)}\circ \bold{e}_j^{(J)}
    \Rightarrow
    \text{vec}(\bold{A})=\sum\limits_{i=1}^{I}\sum\limits_{j=1}^{J}a_{ij}\bold{e}_{(j-1)I+i}^{(JI)},
\end{align*}
with $\bold{e}_{(j-1)I+i}^{(JI)}=\text{vec}(\bold{e}_i^{(I)}\circ \bold{e}_j^{(J)})=\text{vec}(\bold{e}_i^{(I)}(\bold{e}_j^{(J)})^T)$.

\begin{remark}
    Since the operator $\text{vec}$ satisfies $\text{vec}(\alpha \bold{A}+\beta \bold{B})=\alpha \text{vec}(\bold{A})+\beta \text{vec}(\bold{B})$
    for all $\alpha,\beta\in\K$, it is linear.
\end{remark}


\section{Vector inner product and orthogonality}
\subsection{Inner product}

In this section, we recall the definition of the inner product(also called dot product)
of two vectors $\bold{a},\bold{b}\in\K^{I}$.
\begin{definition}{}{}
    If $\K=\R$, the vector inner product is defined as:
    \begin{align*}
        \inner{\cdot}{\cdot}:\R^I\times \R^I&\rightarrow \R\\
        (\bold{a},\bold{b})&\mapsto \inner{\bold{a}}{\bold{b}} = \bold{a}^T\bold{b}=\sum\limits_{i=1}^{I}a_ib_i.
    \end{align*}
    In $\C^I$, the definition of the vector inner product is given by:
    \begin{align*}
        \inner{\cdot}{\cdot}:\C^I\times \C^I&\rightarrow \C\\
        (\bold{a},\bold{b})&\mapsto \inner{\bold{a}}{\bold{b}} = \bold{a}^H\bold{b}=\sum\limits_{i=1}^{I}a_i^*b_i.
    \end{align*}
\end{definition}
\begin{remark}
    Whether $\K=\C$ or $\R$, $\inner{\bold{a}}{\bold{a}}\in \R$. 
\end{remark}


\subsection{Orthogonality}
\begin{definition}{}{}
    Two vectors $\bold{a}$ and $\bold{b}$ of $\K^{I}$ are said to be 
    orthogonal if and only if $\inner{\bold{a}}{\bold{b}}=0$. 
\end{definition}


\section{Vector Norms}
\begin{definition}{}{}
    Let $v:\C^n\rightarrow \R$. 
    Then $v$ is a norm if for all $\bold{x},\bold{y}\in\C^n$
    \begin{itemize}
        \item $\bold{x}\neq 0\Rightarrow v(\bold{x})>0$,
        \item $v(\alpha \bold{x})=|\alpha|v(\bold{x})$, and
        \item $v(\bold{x}+\bold{y})\leqs v(\bold{x})+v(\bold{y})$ 
    \end{itemize}
\end{definition}
\begin{remark}
    often we will use $||\cdot||$ to denote a vector norm.
\end{remark}


\subsection{Vector 2-norm}
\begin{definition}{}{}
    The vector 2-norm $||\cdot||_2:\C^n\rightarrow \R$ is defined by
    \begin{align*}
        ||\bold{x}||_2=\sqrt{\inner{\bold{x}}{\bold{x}}}=\sqrt{\bold{x}^H\bold{x}}=\sqrt{x_1^*x_1+x_2^*x_2+...+x_n^*x_n}=\sqrt{|x_1|^2+...+|x_n|^2}.
    \end{align*}
\end{definition}

\begin{theorem}{}{}
    Let $\bold{x},\bold{y}\in\C^n$. Then $|\inner{\bold{x}}{\bold{y}}|=|\bold{x}^H\bold{y}|\leqs ||\bold{x}||_2\cdot ||\bold{y}||_2$.
\end{theorem}

\begin{proposition}{}{}
    The vector 2-norm is a norm.
\end{proposition}

\subsection{Vector 1-norm}
\begin{definition}{}{}
    The vector 1-norm $||\cdot||_1:\C^n\rightarrow \R$ is defined by
    \begin{align*}
        ||\bold{x}||_1=|x_1|+|x_2|+...+|x_{n}|.
    \end{align*}
\end{definition}

\begin{proposition}{}{}
    The vector 1-norm is a norm.
\end{proposition}

\subsection{Vector $\infty$-norm}
\begin{definition}{}{}
    The vector $\infty$-norm $||\cdot||_{\infty}:\C^n\rightarrow \R$ is defined by $||\bold{x}||_{\infty}=\max_{i}|x_i|$.
\end{definition}

\begin{proposition}{}{}
    The vector $\infty$-norm is a norm.
\end{proposition}

\subsection{Vector $p$-norm}
\begin{definition}{}{}
    The vector $p$-norm $||\cdot||_p:\C^n\rightarrow \R$ is defined by
    \begin{align*}
        ||\bold{x}||_p=\sqrt[p]{|x_1|^p+|x_2|^p+...+|x_n|^p}.
    \end{align*}
\end{definition}

\begin{proposition}{}{}
    The vector $p$-norm is a norm.
\end{proposition}

\section{Matrix Norms}
It is not hard to see that vector norms are
all measures of how "big" the vectors are. Similarly,
we want to have measures for how "big" matrices are.
We will start with one that are somewhat artificial and then move on to the important class of induced matrix norms.

\subsection{Frobenius norm}
\begin{definition}{}{}
    The Frobenius norm $||\cdot||_F:\C^{m\times n}\rightarrow \R$ is defined by
    \begin{align*}
        ||\bold{A}||_F=\sqrt{\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{n}|a_{ij}|^2}.
    \end{align*}
\end{definition}

\begin{remark}
    $||\bold{A}||_F=||\text{vec}(\bold{A})||_2$.
\end{remark}

\begin{proposition}{}{}
    The Frobenius norm is a norm.
\end{proposition}

\subsection{Induced matrix norms}
\begin{definition}{}{}
    Let $||\cdot||_{\mu}:\C^m\rightarrow \R$ and $||\cdot||_{v}:\C^n\rightarrow \R$ be vector norms.
    Define $||\cdot||_{\mu,v}:\C^{m\times n}\rightarrow \R$ by 
    \begin{align*}
        ||\bold{A}||_{\mu,v}=\sup_{\bold{x}\in\C^n,\bold{x}\neq 0}\frac{||\bold{A\bold{x}}||_{\mu}}{||\bold{x}||_v}.
    \end{align*}
\end{definition}

\begin{remark}
    How "big" $\bold{A}$ is, as measured by $||\bold{A}||_{\mu,v}$, 
    is defined as the most that $\bold{A}$ magnifies the length of nonzero vectors,
    where the length of the vector $\bold{x}$ is measured with norm $||\cdot||_v$ and
    the length of the transformed vector $\bold{A}\bold{x}$ is measured with norm $||\cdot||_{\mu}$.
\end{remark}

\begin{proposition}{}{}
    Let $||\cdot||_{\mu}:\C^m\rightarrow \R$ and $||\cdot||_{v}:\C^n\rightarrow \R$ be vector norms.
    \begin{align*}
        ||\bold{A}||_{\mu,v}&=\sup_{\bold{x}\in\C^n,\bold{x}\neq 0}\frac{||\bold{A\bold{x}}||_{\mu}}{||\bold{x}||_v}\\
                            &=\max_{\bold{x}\in\C^n,\bold{x}\neq 0}\frac{||\bold{A\bold{x}}||_{\mu}}{||\bold{x}||_v}\\
                            &=\max_{||\bold{x}||_v=1}||\bold{A}\bold{x}||_{\mu}.
    \end{align*}
\end{proposition}

\begin{proposition}{}{}
    $||\cdot||_{\mu,v}:\C^{m\times n}\rightarrow \R$ is a norm. 
\end{proposition}

\begin{definition}{}{}
    Define $||\cdot||_p:\C^{m\times n}\rightarrow \R$ by
    \begin{align*}
        ||\bold{A}||_{p}&=\max_{\bold{x}\in\C^n,\bold{x}\neq 0}\frac{||\bold{A\bold{x}}||_{p}}{||\bold{x}||_p}\\
                            &=\max_{||\bold{x}||_p=1}||\bold{A}\bold{x}||_{p}.
    \end{align*}
\end{definition}

\begin{proposition}{}{}
    For all $\bold{A}\in \C^{m\times n}, \bold{x}\in \C^n$,
    \begin{align*}
        ||\bold{A}\bold{x}||\leqs ||\bold{A}||_p\cdot ||\bold{x}||_p.
    \end{align*}
\end{proposition}
\begin{proof}
    By the definition of $||\bold{A}||_p$,
    \begin{align*}
        \frac{||\bold{A}||_p}{||\bold{x}||_p}\leqs ||\bold{A}||_p.
    \end{align*}
\end{proof}


\begin{proposition}{}{}
    For any $\bold{A}\in \C^{m\times k}$ and $\bold{B}\in\C^{k\times n}$, 
    \begin{align*}
        ||\bold{A}\bold{B}||_p&\leqs ||\bold{A}||_p||\bold{B}||_p\\
        ||\bold{A}\bold{B}||_F&\leqs ||\bold{A}||_F||\bold{B}||_F.
    \end{align*}
\end{proposition}

\section{Matrix multiplication}
Suppose $\bold{A}\in\R^{m\times r}$ and $\bold{B}\in \R^{r\times n}$,
the matrix multiplication $\bold{C}=\bold{A}\bold{B}$ can be viewed from
three different perspectives as follows:
\par
\underline{Dot Product Matrix Multiply}. 
Every element $c_{ij}$ of $\bold{C}$ is the 
dot product of row vector $\bold{A}_{i\cdot}$ and column vector $\bold{B}_{\cdot j}$.
\begin{align*}
    \bold{A}&=\begin{pmatrix}
        \bold{A}_{1\cdot}^T\\ ...\\\bold{A}_{m\cdot}^T
    \end{pmatrix},\bold{A}_{k\cdot}\in\R^r\\
    \bold{B}&=\begin{pmatrix}
        \bold{B}_{\cdot 1}&...\bold{B}_{\cdot n}
    \end{pmatrix},\bold{B}_{\cdot k}\in\R^r\\
    \bold{C}&=(c_{ij}),c_{ij}=\bold{A}_{i\cdot}^T\bold{B}_{\cdot j}=\sum\limits_{k=1}^{r}a_{ik}b_{kj}.
\end{align*}
\par
\underline{Column Combination Matrix Multiply}.
Every column $\bold{C}_{\cdot j}$ of $\bold{C}$ is a linear combination of column vector
$\bold{A}_{\cdot k}$ of $\bold{A}$ with columns $b_{kj}$ as the weight coefficients.






\section{Matrix trace, Matrix inner product}

\subsection{Definition and properties of the trace}
\begin{definition}{}{}
    The trace of a square matrix $\bold{A}$ of order $I$ is defined as the sum of its diagonal elements:
    \begin{align*}
        \text{tr}(\bold{A})=\sum\limits_{i=1}^{I}a_{ii}.
    \end{align*}
\end{definition}

\begin{proposition}{}{}
    The trace satisfies the following properties:
    \begin{align*}
        \text{tr}(\alpha \bold{A}+\beta \bold{B}) &= \alpha \text{tr}(\bold{A}) + \beta \text{tr}(\bold{B}),\\
        \text{tr}(\bold{A}^T)&=\text{tr}(\bold{A}),\\
        \text{tr}(\bold{A}^*)&=\text{tr}(\bold{A}^H)=(\text{tr}(\bold{A}))^*,\\
    \end{align*}
\end{proposition}


\section{Eigenvalues and eigenvectors}
\begin{definition}{}{}
    A real matrix $\bold{A}$ is a symmetric matrix 
    if it equals to its own transpose, 
    that is $\bold{A} = \bold{A}^T$.
\end{definition}

\begin{definition}{}{}
    A complex matrix $\bold{A}$ is a hermitian matrix 
    if it equals to its own complex conjugate transpose, 
    that is $\bold{A} = \bold{A}^H$.
\end{definition}

\begin{definition}{}{}
    A real matrix $\bold{Q}$ is an orthogonal matrix 
    if the inverse of $\bold{Q}$ equals to 
    the transpose of $\bold{Q}$, $\bold{Q}^{-1} = \bold{Q}^T$
    , that is $\bold{Q}\bold{Q}^T = \bold{Q}^T\bold{Q} = I$.
\end{definition}

\begin{definition}{}{}
    A complex matrix $\bold{U}$ is a unitary matrix 
    if the inverse of $\bold{U}$ equals the complex conjugate
    transpose of $\bold{U}$, $\bold{U}^{-1} = \bold{U}^H$, 
    that is $\bold{U}\bold{U}^H = \bold{U}^H\bold{U} = \bold{I}$.
\end{definition}

\begin{definition}{}{}
    A hermitian matrix $\bold{Q}$ is positive semidefinite (abbreviated SPSD and denoted by $\bold{Q}\succeq 0$) if 
    \begin{align*}
        \bold{x}^H\bold{Q}\bold{x}\geqs 0 \text{ for all } \bold{x}\in \C^n.
    \end{align*}
\end{definition}

\begin{definition}{}{}
    A hermitian matrix $\bold{Q}$ is positive semidefinite (abbreviated SPD and denoted by $\bold{Q}\succ 0$) if 
    \begin{align*}
        \bold{x}^H\bold{Q}\bold{x}> 0 \text{ for all } \bold{x}\in \C^n, \bold{x}\neq 0.
    \end{align*}
\end{definition}

A number $\lambda\in\C$ is an eigenvalue of $\bold{M}$ if there exists 
a vector $\bold{\bar{x}}\neq 0$ such that $\bold{M}\bold{\bar{x}}=\lambda \bold{\bar{x}}$.
$\bold{\bar{x}}$ is called an eigenvector of $\bold{M}$ (and is called an eigenvector corresponding to $\lambda$).
Note that $\lambda$ is an eigenvalue of $\bold{M}$ if and only if there exists $\bold{\bar{x}}\neq 0$ such that 
$(\bold{M}-\lambda \bold{E})\bold{\bar{x}}=0$ or, equivalently, if and only if $\text{det}(\bold{M}-\lambda \bold{E})=0$.

Let $g(\lambda)=\text{det}(\bold{M}-\lambda \bold{E})$. Then $g(\lambda)$ is 
a polynomial of degree $n$, and so will have $n$ roots 
that will solve the equation $g(\lambda)=\text{det}(\bold{M}-\lambda \bold{E})=0$, 
including multiplicities. These roots are the eigenvalues of $\bold{M}$.


\begin{proposition}{}{}
    If $\bold{Q}$ is a real symmetric matrix, all of its eigenvalues are real numbers.
\end{proposition}

\begin{proposition}{}{}
    If $\bold{Q}$ is a complex hermitian matrix, all of its eigenvalues are real numbers.
\end{proposition}

\begin{proposition}{}{}
    If $\bold{Q}$ is a hermitian matrix, 
    its eigenvectors corresponding to different eigenvalues are orthogonal.
\end{proposition}

\begin{proposition}{}{}
    If $\bold{Q}$ is SPSD, the eigenvalues of $\bold{Q}$ are nonnegative.
\end{proposition}

\begin{theorem}{}{}
    If $\bold{A}$ is a real symmetric matrix, then $\bold{A}=\bold{Q}\bold{D}\bold{Q}^T$,
    where $\bold{Q}$ is an orthonormal matrix, the columns of $\bold{Q}$ are an orthonormal basis of eigenvectors of $\bold{A}$,
    and $\bold{D}$ is a diagonal matrix of the corresponding eigenvalues of $\bold{A}$.
\end{theorem}

\begin{theorem}{}{}
    If $\bold{A}$ is a complex hermitian matrix, then $\bold{A}=\bold{U}\bold{D}\bold{U}^H$,
    where $\bold{U}$ is an unitary matrix, the columns of $\bold{U}$ are an orthonormal basis of eigenvectors of $\bold{A}$,
    and $\bold{D}$ is a diagonal matrix of the corresponding eigenvalues of $\bold{A}$.
\end{theorem}

\begin{proposition}{}{}
    If $\bold{D}$ is SPSD, the $\bold{Q}=\bold{M}^H\bold{M}$ for some matrix $\bold{M}$.
\end{proposition}

\begin{definition}{}{}
    The Rayleigh quotient of the matrix $\bold{A}\in \C^{n\times n}$ at the nonzero vector $\bold{x}\in \C^{n}$
    is the scalar
    \begin{align*}
        \frac{\bold{x}^H\bold{A}\bold{x}}{\bold{x}^H\bold{x}}\in \C.
    \end{align*}
\end{definition}
\begin{remark}
    If $(\lambda,\bold{u})$ is an eigenpair for $\bold{A}$, then notice that
    \begin{align*}
        \frac{\bold{u}^H\bold{A}\bold{u}}{\bold{u}^H\bold{u}}=\frac{\bold{u}^H(\lambda \bold{u})}{\bold{u}^H\bold{u}}=\lambda,
    \end{align*}
    so Rayleigh quotients generalize eigenvalues.
\end{remark}
\begin{remark}
    For Hermitian $\bold{A}\in \C^{n\times n}$, 
    then there exists unitary matrix $\bold{U}$ such that $A=\bold{U}\Lambda \bold{U}^H$, 
    for any $\bold{x}\neq \bold{0}$,  it can be represented by $\bold{x}=\bold{U}\bold{c}=\sum\limits_{j=1}^{n}c_j\bold{u}_j$, then 
    \begin{align*}
        \frac{\bold{x}^H\bold{A}\bold{x}}{\bold{x}^H\bold{x}}
        =\frac{\bold{c}^H\bold{U}^H\bold{U}\Lambda \bold{U}^H\bold{U}\bold{c}}{\bold{c}^H\bold{U}^H\bold{U}\bold{c}}
        =\frac{c^H\Lambda c}{c^Hc}
    \end{align*}
    The diagonal
    structure of $\Lambda$ allows for an illuminating refinement,
    \begin{align*}
        \frac{\bold{x}^H\bold{A}\bold{x}}{\bold{x}^H\bold{x}}=\frac{\lambda_1 |c_1|^2+...+\lambda_n |c_n|^2}{|c_1|^2+...+|c_n|^2}.
    \end{align*}
    As the numerator and denominator are both real, notice that the Rayleigh
    quotients for a Hermitian matrix is always real. We can say more: if the eigenvalues are ordered, $\lambda_1\leqs ... \leqs \lambda_n$,
    \begin{align*}
        \frac{\bold{x}^H\bold{A}\bold{x}}{\bold{x}^H\bold{x}}=\frac{\lambda_1 |c_1|^2+...+\lambda_n |c_n|^2}{|c_1|^2+...+|c_n|^2}\geqs \frac{\lambda_1 (|c_1|^2+...+|c_n|^2)}{|c_1|^2+...+|c_n|^2}=\lambda_1,
    \end{align*}
    and similarly,
    \begin{align*}
        \frac{\bold{x}^H\bold{A}\bold{x}}{\bold{x}^H\bold{x}}=\frac{\lambda_1 |c_1|^2+...+\lambda_n |c_n|^2}{|c_1|^2+...+|c_n|^2}\leqs \frac{\lambda_n (|c_1|^2+...+|c_n|^2)}{|c_1|^2+...+|c_n|^2}=\lambda_n.
    \end{align*}
\end{remark}

\begin{proposition}{}{the range of the Rayleigh quotient}
    For a Hermitian matrix $\bold{A}\in \C^{n\times n}$ with eigenvalues $\lambda_1\leqs ...\leqs \lambda_n$,
    the Rayleigh quotient for nonzero $\bold{x}\in \C^{n\times n}$ satisfies
    \begin{align*}
        \frac{\bold{x}^H\bold{A}\bold{x}}{\bold{x}^H\bold{x}}\in [\lambda_1,\lambda_n].
    \end{align*}
\end{proposition}

\begin{proposition}{}{}
    If $\bold{A}\in \C^{m\times n}$ with $m<n$ and $\bold{A}$ has rank $m$, then 
    \begin{align*}
        ||\bold{A}||_2=\sqrt{\lambda_{\max}(\bold{A}^H\bold{A})}=\sqrt{\lambda_{\max}(\bold{A}\bold{A}^H)},
    \end{align*}
    where $\lambda_{\max}(\bold{M})$ denotes the largest eigenvalue of a matrix $\bold{M}$.
\end{proposition}

\begin{proof}
    \begin{align*}
        ||\bold{A}||_2^2=\sup_{\bold{x}\in\C^n,||\bold{x}||\neq 0} \frac{||\bold{Ax}||_2^2}{||\bold{x}||_2^2}
        =\sup_{\bold{x}\in\C^n,||\bold{x}||\neq 0}\frac{\inner{\bold{Ax}}{\bold{Ax}}}{\inner{\bold{x}}{\bold{x}}}
        =\sup_{\bold{x}\in\C^n,||\bold{x}||\neq 0}\frac{\bold{x}\bold{A}^H\bold{A}\bold{x}}{\bold{x}^H\bold{x}}
        =\lambda_{\text{max}}(\bold{A}^H\bold{A}).
    \end{align*}
\end{proof}

\begin{proposition}{}{}
    If $\bold{A}\in \C^{n\times n}$ is a hermitian matrix, then 
    \begin{align*}
        ||\bold{A}||_2= |\lambda_{\max}(\bold{A})|.
    \end{align*}
\end{proposition}

\begin{proof}
    Since $\bold{A}$ is a hermitian matrix, we have $\bold{A}=\bold{U}^H\Lambda \bold{U}$ where $\bold{U}$ is an unitary matrix and $\Lambda$ is a diagonal matrix containing the eigenvalue of $\bold{A}$.
    Let $\bold{y}=\bold{Ux}$, then $||\bold{y}||_2=||\bold{Ux}||_2$. Hence 
    \begin{align*}
        ||\bold{A}||_2^2=\max_{||\bold{x}||_2=1} ||\bold{Ax}||_2^2 = \max_{||\bold{x}||_2=1} \inner{\bold{Ax}}{\bold{Ax}}
        = \max_{||\bold{x}||_2=1} \bold{x}^H\bold{A}^H\bold{A}\bold{x} =\max_{||\bold{y}||_2=1} \bold{y}^H\Lambda\bold{y}=(\lambda_{\max}(\bold{A}))^2.
    \end{align*}
\end{proof}


\begin{proposition}{}{}
    Suppose that $\bold{A}\in\R^{n\times n}$ is a SPSD. Then the following are equivalent:
    \begin{align*}
        &\text{(a) } h>0 \text{ satisfies } ||\bold{A}^{-1}||_2\leqs \frac{1}{h}.\\
        &\text{(b) } h>0 \text{ satisfies } ||\bold{A}\bold{x}||_2 \geqs h\cdot ||\bold{x}||_2 \text{ for any vector } \bold{x}\\
        &\text{(c) } h>0 \text{ satisfies } |\lambda_i(\bold{A})|\geqs h \text{ for every eigenvalue } \lambda_i(\bold{A}) \text{ of } \bold{A}, i=1,...,m.
    \end{align*}
\end{proposition}
\begin{proof}
    By proposition \ref{prop:the range of the Rayleigh quotient}, 
    we have for all $\bold{x}\neq 0$,
    \begin{align*}
        \frac{||\bold{Ax}||_2}{||\bold{x}||_2}\in [\lambda_{\min}(\bold{A}),\lambda_{\max}(\bold{A})].
    \end{align*}
    Firstly, we claim that 
    \begin{align*}
        ||\bold{A}^{-1}||_2 =  \frac{1}{\lambda_{\min}(\bold{A})}.
    \end{align*}
    In fact, 
    \begin{align*}
        ||\bold{A}^{-1}||_2 &=  \max_{\bold{x}\neq 0} \frac{||\bold{A}^{-1}\bold{x}||_2}{||\bold{x}||_2}
        =\max_{\bold{Ax}\neq 0} \frac{||\bold{A}^{-1}\bold{A}x||_2}{||\bold{Ax}||_2} \\
        &=\max_{\bold{Ax}\neq 0} \frac{||\bold{x}||_2}{||\bold{Ax}||_2}
        = \max_{\bold{x}\neq 0} \frac{||\bold{x}||_2}{||\bold{Ax}||_2}\\
        &=\frac{1}{\min_{\bold{x}\neq 0} \frac{||\bold{Ax}||_2}{||\bold{x}||_2}}
        =\frac{1}{\lambda_{\min}(\bold{A})},
    \end{align*}
    the second and forth equality follows from the fact that $\bold{x}\neq 0$
    if and only if $\bold{Ax}\neq 0$ since $\bold{A}$ is nonsingular. 
    \par
    (a) $\Rightarrow$ (b) \quad $\frac{1}{\lambda_{\min}(\bold{A})}=||\bold{A}^{-1}||_2\leqs \frac{1}{h}$
    $\Rightarrow \frac{||\bold{Ax}||_2}{||\bold{x}||_2}\geqs \lambda_{\min}(\bold{A})\geqs h$, $\forall \bold{x}\neq 0$.
    \par
    (b) $\Rightarrow$ (a) \quad $||\bold{Ax}||_2\geqs h\cdot ||x||_2$, $\forall \bold{x}$ $\Rightarrow$ $\lambda_{\min}(\bold{A})\geqs h$
    $\Rightarrow$ $\frac{1}{\lambda_{\min}(\bold{A})} = ||\bold{A}^{-1}||_2\leqs \frac{1}{h}$.
    \par
    (b) $\Leftrightarrow$ (c) \quad $||\bold{Ax}||_2\geqs h\cdot ||x||_2$, $\forall \bold{x}$ $\Leftrightarrow$ $\lambda_{\min}(\bold{A})\geqs h$
    $\Leftrightarrow$ $\lambda_i(\bold{A})\geqs h$, $i=1,...,m$.
    \par
\end{proof}


\section{Generalized inverses}

\section{Reference}
\begin{itemize}
    \item \href{}{From Algebraic Structures to Tensors ch4 matrix algebra}
    \item \href{https://www.cs.utexas.edu/users/flame/Notes/NotesOnNorms.pdf}{Notes on Vector and Matrix Norms}
    \item \href{https://ocw.mit.edu/courses/15-084j-nonlinear-programming-spring-2004/resources/lec4_quad_form/}{symmetric, positive definted, eigenvalues and eigenvectors}
    \item \href{https://www.cmor-faculty.rice.edu/~caam440/chapter2.pdf}{Rayleigh quotient}
    \item \href{https://www.math.uwaterloo.ca/~jmckinno/Math225/Week7/Lecture2m.pdf}{The Principal Axis Theorem}
    \item \href{https://staff.polito.it/ada.boralevi/didattica/Dispense_ENG.pdf}{linear algebra and geometry note}
    \item \href{http://jde27.uk/la/36_eigenapplications2.html}{eigenvalues application: ellipses}
    \item \href{https://www.sjsu.edu/faculty/guangliang.chen/Math253S20/lec6ginverse.pdf}{Generalized inverses}
\end{itemize}
